package org.hjug.feedback.vertex.kernelized;

import com.google.common.util.concurrent.AtomicDouble;
import java.util.*;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;
import java.util.stream.IntStream;
import org.hjug.feedback.SuperTypeToken;
import org.jgrapht.Graph;
import org.jgrapht.Graphs;
import org.jgrapht.alg.connectivity.ConnectivityInspector;
import org.jgrapht.graph.DefaultEdge;
import org.jgrapht.graph.DefaultUndirectedGraph;

/**
 * Multithreaded modulator computer that finds treewidth-η modulators
 * based on the algorithms described in the DFVS paper.
 * Generated by Perplexity.ai's Research model
 */
public class ModulatorComputer<V, E> {

    private final TreewidthComputer<V, E> treewidthComputer;
    private final FeedbackVertexSetComputer<V, E> fvsComputer;
    private final ExecutorService executorService;

    public ModulatorComputer(SuperTypeToken<E> edgeTypeToken) {
        this.treewidthComputer = new TreewidthComputer<>();
        this.fvsComputer = new FeedbackVertexSetComputer<>(edgeTypeToken);
        this.executorService = ForkJoinPool.commonPool();
    }

    public ModulatorComputer(SuperTypeToken<E> edgeTypeToken, int parallelismLevel) {
        this.treewidthComputer = new TreewidthComputer<>(parallelismLevel);
        this.fvsComputer = new FeedbackVertexSetComputer<>(edgeTypeToken, parallelismLevel);
        this.executorService = Executors.newWorkStealingPool(parallelismLevel);
    }

    /**
     * Computes an optimal treewidth-η modulator using multiple strategies
     */
    public ModulatorResult<V> computeModulator(Graph<V, E> graph, int targetTreewidth, int maxModulatorSize) {
        if (maxModulatorSize <= 0) {
            return new ModulatorResult<>(new HashSet<>(), treewidthComputer.computeEta(graph, new HashSet<>()), 0);
        }

        // Run multiple modulator finding strategies in parallel
        List<Callable<Set<V>>> strategies = Arrays.asList(
                () -> computeGreedyDegreeModulator(graph, targetTreewidth, maxModulatorSize),
                () -> computeFeedbackVertexSetModulator(graph, targetTreewidth, maxModulatorSize),
                () -> computeTreewidthDecompositionModulator(graph, targetTreewidth, maxModulatorSize),
                () -> computeHighDegreeVertexModulator(graph, targetTreewidth, maxModulatorSize),
                () -> computeBottleneckVertexModulator(graph, targetTreewidth, maxModulatorSize));

        try {
            List<Future<Set<V>>> results = executorService.invokeAll(strategies, 60, TimeUnit.SECONDS);

            return results.parallelStream()
                    .map(this::getFutureValue)
                    .filter(Objects::nonNull)
                    .filter(modulator -> modulator.size() <= maxModulatorSize && !modulator.isEmpty())
                    .map(modulator -> new ModulatorResult<>(
                            modulator,
                            treewidthComputer.computeEta(graph, modulator),
                            computeModulatorQuality(graph, modulator, targetTreewidth)))
                    .filter(result -> result.getResultingTreewidth() <= targetTreewidth)
                    .min(Comparator.comparingDouble(ModulatorResult::getQualityScore))
                    .orElse(computeFallbackModulator(graph, targetTreewidth, maxModulatorSize));

        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            return computeFallbackModulator(graph, targetTreewidth, maxModulatorSize);
        }
    }

    /**
     * Computes modulator using iterative vertex removal based on degree
     */
    private Set<V> computeGreedyDegreeModulator(Graph<V, E> graph, int targetTreewidth, int maxSize) {
        Set<V> modulator = ConcurrentHashMap.newKeySet();
        Graph<V, DefaultEdge> workingGraph = convertToUndirected(graph);

        while (modulator.size() < maxSize) {
            int currentTreewidth = treewidthComputer.computeEta(graph, modulator);
            if (currentTreewidth <= targetTreewidth) {
                break;
            }

            Optional<Map.Entry<V, Double>> bestVertex =
                    computeVertexRemovalScore(workingGraph, targetTreewidth).entrySet().parallelStream()
                            .max(Map.Entry.comparingByValue());

            if (bestVertex == null || bestVertex.isEmpty()) break;

            modulator.add(bestVertex.get().getKey());
            workingGraph.removeVertex(bestVertex.get().getKey());
        }

        return modulator;
    }

    /**
     * Uses feedback vertex set as starting point for modulator
     */
    private Set<V> computeFeedbackVertexSetModulator(Graph<V, E> graph, int targetTreewidth, int maxSize) {
        Set<V> modulator = new HashSet<>();

        // Start with feedback vertex set vertices (they're often good modulator candidates)
        Set<V> fvs = fvsComputer.greedyFeedbackVertexSet(graph);

        // Add FVS vertices up to budget
        Iterator<V> fvsIter = fvs.iterator();
        while (fvsIter.hasNext() && modulator.size() < maxSize) {
            V vertex = fvsIter.next();
            modulator.add(vertex);

            int currentTreewidth = treewidthComputer.computeEta(graph, modulator);
            if (currentTreewidth <= targetTreewidth) {
                break;
            }
        }

        // If still not good enough, add high-degree vertices
        if (modulator.size() < maxSize) {
            List<V> remainingVertices = graph.vertexSet().stream()
                    .filter(v -> !modulator.contains(v))
                    .sorted((v1, v2) -> Integer.compare(
                            graph.inDegreeOf(v2) + graph.outDegreeOf(v2), graph.inDegreeOf(v1) + graph.outDegreeOf(v1)))
                    .collect(Collectors.toList());

            for (V vertex : remainingVertices) {
                if (modulator.size() >= maxSize) break;

                modulator.add(vertex);
                int currentTreewidth = treewidthComputer.computeEta(graph, modulator);
                if (currentTreewidth <= targetTreewidth) {
                    break;
                }
            }
        }

        return modulator;
    }

    /**
     * Uses treewidth decomposition analysis to find modulator
     */
    private Set<V> computeTreewidthDecompositionModulator(Graph<V, E> graph, int targetTreewidth, int maxSize) {
        Set<V> modulator = ConcurrentHashMap.newKeySet();
        Graph<V, DefaultEdge> undirected = convertToUndirected(graph);

        // Identify vertices that appear in many high-width bags
        Map<V, Integer> bagAppearances = new ConcurrentHashMap<>();
        Map<V, Double> centralityScores = computeBetweennessCentralityParallel(undirected);

        // Compute vertex importance based on structural properties
        Map<V, Double> vertexImportance = undirected.vertexSet().parallelStream()
                .collect(Collectors.toConcurrentMap(
                        v -> v,
                        v -> computeStructuralImportance(undirected, v, centralityScores.getOrDefault(v, 0.0))));

        // Greedily select vertices with highest importance
        List<V> sortedVertices = vertexImportance.entrySet().stream()
                .sorted(Map.Entry.<V, Double>comparingByValue().reversed())
                .map(Map.Entry::getKey)
                .collect(Collectors.toList());

        for (V vertex : sortedVertices) {
            if (modulator.size() >= maxSize) break;

            modulator.add(vertex);
            int currentTreewidth = treewidthComputer.computeEta(graph, modulator);
            if (currentTreewidth <= targetTreewidth) {
                break;
            }
        }

        return modulator;
    }

    /**
     * Focuses on highest degree vertices first
     */
    private Set<V> computeHighDegreeVertexModulator(Graph<V, E> graph, int targetTreewidth, int maxSize) {
        Set<V> modulator = new HashSet<>();

        List<V> verticesByDegree = graph.vertexSet().stream()
                .sorted((v1, v2) -> Integer.compare(
                        graph.inDegreeOf(v2) + graph.outDegreeOf(v2), graph.inDegreeOf(v1) + graph.outDegreeOf(v1)))
                .collect(Collectors.toList());

        for (V vertex : verticesByDegree) {
            if (modulator.size() >= maxSize) break;

            modulator.add(vertex);
            int currentTreewidth = treewidthComputer.computeEta(graph, modulator);
            if (currentTreewidth <= targetTreewidth) {
                break;
            }
        }

        return modulator;
    }

    /**
     * Identifies bottleneck vertices that connect different components
     */
    private Set<V> computeBottleneckVertexModulator(Graph<V, E> graph, int targetTreewidth, int maxSize) {
        Set<V> modulator = ConcurrentHashMap.newKeySet();
        Graph<V, DefaultEdge> undirected = convertToUndirected(graph);

        // Find articulation points and vertices with high betweenness centrality
        Set<V> articulationPoints = findArticulationPoints(undirected);
        Map<V, Double> centralityScores = computeBetweennessCentralityParallel(undirected);

        // Combine articulation points with high centrality vertices
        Set<V> candidates = new HashSet<>(articulationPoints);
        candidates.addAll(centralityScores.entrySet().stream()
                .sorted(Map.Entry.<V, Double>comparingByValue().reversed())
                .limit(Math.max(10, maxSize * 2))
                .map(Map.Entry::getKey)
                .collect(Collectors.toSet()));

        // Greedily select best candidates
        for (V vertex : candidates) {
            if (modulator.size() >= maxSize) break;

            modulator.add(vertex);
            int currentTreewidth = treewidthComputer.computeEta(graph, modulator);
            if (currentTreewidth <= targetTreewidth) {
                break;
            }
        }

        return modulator;
    }

    /**
     * Computes vertex removal scores based on their impact on achieving the target treewidth.
     *
     * This method evaluates vertices based on multiple criteria:
     * 1. Direct treewidth reduction impact
     * 2. Degree-based scoring relative to target treewidth
     * 3. Structural importance (betweenness centrality, clustering coefficient)
     * 4. Connectivity disruption potential
     * 5. Distance from target treewidth achievement
     *
     * @param targetTreewidth the desired treewidth after vertex removal
     * @return concurrent map of vertices to their removal scores (higher = more beneficial to remove)
     */
    public ConcurrentHashMap<V, Double> computeVertexRemovalScore(Graph<V, DefaultEdge> graph, int targetTreewidth) {
        Set<V> vertices = graph.vertexSet();
        int n = vertices.size();

        if (n == 0 || targetTreewidth < 0) {
            return new ConcurrentHashMap<>();
        }

        // Initialize concurrent data structures
        ConcurrentHashMap<V, Double> scores = new ConcurrentHashMap<>();
        ConcurrentHashMap<V, Integer> degrees = new ConcurrentHashMap<>();
        ConcurrentHashMap<V, Double> structuralImportance = new ConcurrentHashMap<>();

        // Custom thread pool for optimal performance
        ForkJoinPool customThreadPool =
                new ForkJoinPool(Math.min(Runtime.getRuntime().availableProcessors(), Math.max(1, n / 100)));

        try {
            CompletableFuture<Void> computation = CompletableFuture.runAsync(
                    () -> {
                        // Phase 1: Compute basic metrics in parallel
                        computeBasicMetricsParallel(graph, vertices, degrees, targetTreewidth);

                        // Phase 2: Compute structural importance in parallel
                        computeStructuralImportanceParallel(graph, vertices, structuralImportance, targetTreewidth);

                        // Phase 3: Compute comprehensive scores in parallel
                        computeComprehensiveScoresParallel(
                                graph, vertices, scores, degrees, structuralImportance, targetTreewidth);

                        // Phase 4: Apply target treewidth specific adjustments
                        applyTargetTreewidthAdjustmentsParallel(graph, vertices, scores, targetTreewidth);
                    },
                    customThreadPool);

            computation.get();

        } catch (InterruptedException | ExecutionException e) {
            Thread.currentThread().interrupt();
            throw new RuntimeException("Parallel vertex scoring computation failed", e);
        } finally {
            shutdownThreadPool(customThreadPool);
        }

        return scores;
    }

    /**
     * Computes basic graph metrics in parallel for vertex scoring.
     */
    private void computeBasicMetricsParallel(
            Graph<V, DefaultEdge> graph, Set<V> vertices, ConcurrentHashMap<V, Integer> degrees, int targetTreewidth) {

        // Compute degrees in parallel
        vertices.parallelStream().forEach(vertex -> {
            int degree = graph.inDegreeOf(vertex) + graph.outDegreeOf(vertex);
            degrees.put(vertex, degree);
        });
    }

    /**
     * Computes structural importance metrics in parallel.
     */
    private void computeStructuralImportanceParallel(
            Graph<V, DefaultEdge> graph,
            Set<V> vertices,
            ConcurrentHashMap<V, Double> structuralImportance,
            int targetTreewidth) {

        // Compute structural metrics in parallel
        vertices.parallelStream().forEach(vertex -> {
            double importance = 0.0;

            // Factor 1: Local clustering coefficient impact
            importance += computeLocalClusteringImpact(graph, vertex, targetTreewidth);

            // Factor 2: Connectivity importance
            importance += computeConnectivityImportance(graph, vertex, targetTreewidth);

            // Factor 3: Neighborhood density impact
            importance += computeNeighborhoodDensityImpact(graph, vertex, targetTreewidth);

            structuralImportance.put(vertex, importance);
        });
    }

    /**
     * Computes comprehensive removal scores incorporating all factors and target treewidth.
     */
    private void computeComprehensiveScoresParallel(
            Graph<V, DefaultEdge> graph,
            Set<V> vertices,
            ConcurrentHashMap<V, Double> scores,
            ConcurrentHashMap<V, Integer> degrees,
            ConcurrentHashMap<V, Double> structuralImportance,
            int targetTreewidth) {

        // Compute statistics for normalization
        DoubleSummaryStatistics degreeStats = degrees.values().parallelStream()
                .mapToDouble(Integer::doubleValue)
                .summaryStatistics();

        DoubleSummaryStatistics importanceStats = structuralImportance.values().parallelStream()
                .mapToDouble(Double::doubleValue)
                .summaryStatistics();

        // Compute comprehensive scores in parallel
        vertices.parallelStream().forEach(vertex -> {
            double score = 0.0;
            int degree = degrees.get(vertex);
            double importance = structuralImportance.get(vertex);

            // Component 1: Degree-based score relative to target treewidth
            score += computeDegreeBasedScore(degree, targetTreewidth, degreeStats);

            // Component 2: Structural importance score
            score += computeNormalizedImportanceScore(importance, importanceStats);

            // Component 3: Target treewidth proximity score
            score += computeTargetProximityScore(graph, vertex, degree, targetTreewidth);

            // Component 4: Treewidth reduction potential
            score += computeTreewidthReductionPotential(graph, vertex, targetTreewidth);

            // Component 5: Graph connectivity preservation penalty
            score -= computeConnectivityPreservationPenalty(graph, vertex, targetTreewidth);

            scores.put(vertex, score);
        });
    }

    /**
     * Computes degree-based score considering the target treewidth.
     * Higher degree vertices that exceed target treewidth get higher scores.
     */
    private double computeDegreeBasedScore(int degree, int targetTreewidth, DoubleSummaryStatistics degreeStats) {

        // Normalize degree
        double normalizedDegree = degreeStats.getMax() > degreeStats.getMin()
                ? (degree - degreeStats.getMin()) / (degreeStats.getMax() - degreeStats.getMin())
                : 0.0;

        // Base score from normalized degree
        double baseScore = normalizedDegree;

        // Boost score if degree significantly exceeds target treewidth
        if (degree > targetTreewidth) {
            double excess = (double) (degree - targetTreewidth) / Math.max(1, targetTreewidth);
            baseScore *= (1.0 + excess); // Amplify score for high-degree vertices
        }

        // Penalty if degree is already below or at target
        else if (degree <= targetTreewidth) {
            double deficit = (double) (targetTreewidth - degree) / Math.max(1, targetTreewidth);
            baseScore *= (1.0 - deficit * 0.5); // Reduce score but don't eliminate
        }

        return baseScore * 0.3; // Weight: 30% of total score
    }

    /**
     * Computes local clustering coefficient impact on treewidth.
     */
    private double computeLocalClusteringImpact(Graph<V, DefaultEdge> graph, V vertex, int targetTreewidth) {
        Set<V> neighbors = getNeighbors(vertex, graph);

        if (neighbors.size() < 2) {
            return 0.0;
        }

        // Count edges among neighbors
        AtomicInteger edgeCount = new AtomicInteger(0);
        List<V> neighborList = new ArrayList<>(neighbors);

        neighborList.parallelStream().forEach(n1 -> {
            int index1 = neighborList.indexOf(n1);
            neighborList.stream()
                    .skip(index1 + 1)
                    .filter(n2 -> graph.containsEdge(n1, n2) || graph.containsEdge(n2, n1))
                    .forEach(n2 -> edgeCount.incrementAndGet());
        });

        int maxPossibleEdges = neighbors.size() * (neighbors.size() - 1) / 2;
        double clusteringCoefficient = maxPossibleEdges > 0 ? (double) edgeCount.get() / maxPossibleEdges : 0.0;

        // High clustering + high degree suggests clique-like structures that increase treewidth
        double impact = clusteringCoefficient * Math.min(1.0, (double) neighbors.size() / (targetTreewidth + 1));

        return impact;
    }

    /**
     * Computes connectivity importance based on how removal affects graph connectivity.
     */
    private double computeConnectivityImportance(Graph<V, DefaultEdge> graph, V vertex, int targetTreewidth) {
        Set<V> neighbors = getNeighbors(vertex, graph);

        if (neighbors.size() <= 1) {
            return 0.1; // Low importance for low-degree vertices
        }

        // Estimate impact on connectivity
        double connectivityScore = 0.0;

        // Factor 1: Bridge potential (connecting different components)
        connectivityScore += estimateBridgePotential(graph, vertex, neighbors, targetTreewidth);

        // Factor 2: Articulation point potential
        connectivityScore += estimateArticulationPotential(graph, vertex, neighbors, targetTreewidth);

        return Math.min(1.0, connectivityScore);
    }

    /**
     * Estimates if vertex acts as a bridge relative to target treewidth constraints.
     */
    private double estimateBridgePotential(
            Graph<V, DefaultEdge> graph, V vertex, Set<V> neighbors, int targetTreewidth) {
        if (neighbors.size() < 2) {
            return 0.0;
        }

        // Simple heuristic: check if neighbors are well-connected without this vertex
        AtomicInteger interNeighborConnections = new AtomicInteger(0);

        neighbors.parallelStream().forEach(n1 -> {
            long connections = neighbors.parallelStream()
                    .filter(n2 -> !n1.equals(n2))
                    .filter(n2 -> graph.containsEdge(n1, n2) || graph.containsEdge(n2, n1))
                    .count();
            interNeighborConnections.addAndGet((int) connections);
        });

        double expectedConnections = neighbors.size() * (neighbors.size() - 1) / 2.0;
        double actualConnectionRatio =
                expectedConnections > 0 ? interNeighborConnections.get() / (2.0 * expectedConnections) : 0.0;

        // If neighbors are poorly connected, vertex is more important as bridge
        double bridgeScore = 1.0 - actualConnectionRatio;

        // Scale by target treewidth considerations
        double targetFactor = Math.min(1.0, (double) neighbors.size() / Math.max(1, targetTreewidth));

        return bridgeScore * targetFactor;
    }

    /**
     * Estimates articulation point potential.
     */
    private double estimateArticulationPotential(
            Graph<V, DefaultEdge> graph, V vertex, Set<V> neighbors, int targetTreewidth) {
        // Simplified articulation point detection
        if (neighbors.size() < 2) {
            return 0.0;
        }

        // High-degree vertices in sparse neighborhoods are likely articulation points
        double degreeRatio = Math.min(1.0, (double) neighbors.size() / Math.max(1, targetTreewidth));
        double sparsityFactor = computeNeighborhoodSparsity(graph, neighbors);

        return degreeRatio * sparsityFactor;
    }

    /**
     * Computes neighborhood density impact.
     */
    private double computeNeighborhoodDensityImpact(Graph<V, DefaultEdge> graph, V vertex, int targetTreewidth) {
        Set<V> neighbors = getNeighbors(vertex, graph);

        if (neighbors.size() <= targetTreewidth) {
            return 0.2; // Low impact if neighborhood already small
        }

        // Count edges in the neighborhood
        AtomicInteger neighborhoodEdges = new AtomicInteger(0);
        List<V> neighborList = new ArrayList<>(neighbors);

        neighborList.parallelStream().forEach(n1 -> {
            int index1 = neighborList.indexOf(n1);
            long edgeCount = neighborList.stream()
                    .skip(index1 + 1)
                    .parallel()
                    .filter(n2 -> graph.containsEdge(n1, n2) || graph.containsEdge(n2, n1))
                    .count();
            neighborhoodEdges.addAndGet((int) edgeCount);
        });

        int maxPossibleEdges = neighbors.size() * (neighbors.size() - 1) / 2;
        double density = maxPossibleEdges > 0 ? (double) neighborhoodEdges.get() / maxPossibleEdges : 0.0;

        // High density neighborhoods contribute more to treewidth
        double sizeFactor = (double) neighbors.size() / Math.max(1, targetTreewidth);

        return density * Math.min(2.0, sizeFactor);
    }

    /**
     * Computes neighborhood sparsity factor.
     */
    private double computeNeighborhoodSparsity(Graph<V, DefaultEdge> graph, Set<V> neighbors) {
        if (neighbors.size() < 2) {
            return 1.0;
        }

        AtomicInteger edgeCount = new AtomicInteger(0);
        List<V> neighborList = new ArrayList<>(neighbors);

        neighborList.parallelStream().forEach(n1 -> {
            int index1 = neighborList.indexOf(n1);
            long connections = neighborList.stream()
                    .skip(index1 + 1)
                    .parallel()
                    .filter(n2 -> graph.containsEdge(n1, n2) || graph.containsEdge(n2, n1))
                    .count();
            edgeCount.addAndGet((int) connections);
        });

        int maxPossibleEdges = neighbors.size() * (neighbors.size() - 1) / 2;
        double density = maxPossibleEdges > 0 ? (double) edgeCount.get() / maxPossibleEdges : 0.0;

        return 1.0 - density; // Higher sparsity = higher score
    }

    /**
     * Computes normalized importance score.
     */
    private double computeNormalizedImportanceScore(double importance, DoubleSummaryStatistics importanceStats) {
        if (importanceStats.getMax() <= importanceStats.getMin()) {
            return 0.0;
        }

        double normalized =
                (importance - importanceStats.getMin()) / (importanceStats.getMax() - importanceStats.getMin());

        return normalized * 0.25; // Weight: 25% of total score
    }

    /**
     * Computes score based on proximity to target treewidth achievement.
     */
    private double computeTargetProximityScore(Graph<V, DefaultEdge> graph, V vertex, int degree, int targetTreewidth) {
        Set<V> neighbors = getNeighbors(vertex, graph);

        // Estimate local treewidth contribution
        double localTreewidthContribution = Math.max(degree, neighbors.size());

        // Score based on how much this vertex exceeds the target
        if (localTreewidthContribution > targetTreewidth) {
            double excess = (localTreewidthContribution - targetTreewidth) / Math.max(1, targetTreewidth);
            return Math.min(1.0, excess) * 0.25; // Weight: 25% of total score
        }

        return 0.0;
    }

    /**
     * Estimates the potential for treewidth reduction by removing this vertex.
     */
    private double computeTreewidthReductionPotential(Graph<V, DefaultEdge> graph, V vertex, int targetTreewidth) {
        Set<V> neighbors = getNeighbors(vertex, graph);

        if (neighbors.isEmpty()) {
            return 0.1; // Isolated vertices have low reduction potential
        }

        // Estimate reduction potential based on vertex properties
        double potential = 0.0;

        // Factor 1: High-degree vertices in dense neighborhoods
        double degreeContribution = Math.min(1.0, (double) neighbors.size() / (targetTreewidth + 1));
        potential += degreeContribution * 0.4;

        // Factor 2: Vertices that create large cliques when eliminated
        double cliqueFormationPotential = computeCliqueFormationPotential(graph, vertex, neighbors, targetTreewidth);
        potential += cliqueFormationPotential * 0.4;

        // Factor 3: Vertices in high-treewidth substructures
        double substructurePotential = computeSubstructurePotential(graph, vertex, neighbors, targetTreewidth);
        potential += substructurePotential * 0.2;

        return Math.min(1.0, potential) * 0.15; // Weight: 15% of total score
    }

    /**
     * Computes potential for clique formation when vertex is eliminated.
     */
    private double computeCliqueFormationPotential(
            Graph<V, DefaultEdge> graph, V vertex, Set<V> neighbors, int targetTreewidth) {
        if (neighbors.size() <= targetTreewidth) {
            return 0.2; // Low potential if neighborhood already small
        }

        // Estimate how many edges would need to be added to make neighborhood a clique
        AtomicInteger existingEdges = new AtomicInteger(0);
        List<V> neighborList = new ArrayList<>(neighbors);

        neighborList.parallelStream().forEach(n1 -> {
            int index1 = neighborList.indexOf(n1);
            long edgeCount = neighborList.stream()
                    .skip(index1 + 1)
                    .parallel()
                    .filter(n2 -> graph.containsEdge(n1, n2) || graph.containsEdge(n2, n1))
                    .count();
            existingEdges.addAndGet((int) edgeCount);
        });

        int maxPossibleEdges = neighbors.size() * (neighbors.size() - 1) / 2;
        int missingEdges = maxPossibleEdges - existingEdges.get();

        // Higher missing edges = higher potential for treewidth increase if not removed
        double missingRatio = maxPossibleEdges > 0 ? (double) missingEdges / maxPossibleEdges : 0.0;

        // Scale by size relative to target treewidth
        double sizeFactor = Math.min(2.0, (double) neighbors.size() / Math.max(1, targetTreewidth));

        return missingRatio * sizeFactor;
    }

    /**
     * Computes substructure potential impact.
     */
    private double computeSubstructurePotential(
            Graph<V, DefaultEdge> graph, V vertex, Set<V> neighbors, int targetTreewidth) {
        // Simple heuristic: vertices with many high-degree neighbors

        return neighbors.parallelStream()
                        .mapToInt(neighbor -> graph.inDegreeOf(neighbor) + graph.outDegreeOf(neighbor))
                        .filter(degree -> degree > targetTreewidth)
                        .count()
                / (double) Math.max(1, neighbors.size());
    }

    /**
     * Computes penalty for removing vertices that are crucial for connectivity.
     */
    private double computeConnectivityPreservationPenalty(Graph<V, DefaultEdge> graph, V vertex, int targetTreewidth) {
        Set<V> neighbors = getNeighbors(vertex, graph);

        // Penalty for removing vertices that maintain important connections
        double penalty = 0.0;

        // Factor 1: Bridge vertices get higher penalty
        if (isBridgeVertex(graph, vertex, neighbors)) {
            penalty += 0.3;
        }

        // Factor 2: Articulation points get penalty
        if (isLikelyArticulationPoint(graph, vertex, neighbors)) {
            penalty += 0.2;
        }

        // Factor 3: Vertices connecting different high-degree components
        penalty += computeComponentConnectionPenalty(graph, vertex, neighbors, targetTreewidth);

        return Math.min(0.5, penalty); // Cap penalty at 50% of score
    }

    /**
     * Applies target treewidth specific adjustments to scores.
     */
    private void applyTargetTreewidthAdjustmentsParallel(
            Graph<V, DefaultEdge> graph, Set<V> vertices, ConcurrentHashMap<V, Double> scores, int targetTreewidth) {

        // Compute current graph statistics
        DoubleSummaryStatistics scoreStats = scores.values().parallelStream()
                .mapToDouble(Double::doubleValue)
                .summaryStatistics();

        // Apply adjustments in parallel
        vertices.parallelStream().forEach(vertex -> {
            double currentScore = scores.get(vertex);
            double adjustedScore = currentScore;

            // Adjustment 1: Boost vertices that significantly exceed target treewidth
            int degree = graph.inDegreeOf(vertex) + graph.outDegreeOf(vertex);
            if (degree > targetTreewidth * 1.5) {
                adjustedScore *= 1.3; // 30% boost for high-degree vertices
            }

            // Adjustment 2: Normalize relative to target treewidth
            double targetNormalizedFactor =
                    1.0 + (double) Math.max(0, degree - targetTreewidth) / Math.max(1, targetTreewidth);
            adjustedScore *= targetNormalizedFactor;

            // Adjustment 3: Apply final bounds
            adjustedScore = Math.max(0.0, Math.min(10.0, adjustedScore));

            scores.put(vertex, adjustedScore);
        });
    }

    /**
     * Helper method to get all neighbors of a vertex.
     */
    private Set<V> getNeighbors(V vertex, Graph<V, DefaultEdge> graph) {
        Set<V> neighbors = ConcurrentHashMap.newKeySet();

        // Add in-neighbors
        graph.incomingEdgesOf(vertex).parallelStream()
                .map(graph::getEdgeSource)
                .filter(neighbor -> !neighbor.equals(vertex))
                .forEach(neighbors::add);

        // Add out-neighbors
        graph.outgoingEdgesOf(vertex).parallelStream()
                .map(graph::getEdgeTarget)
                .filter(neighbor -> !neighbor.equals(vertex))
                .forEach(neighbors::add);

        return neighbors;
    }

    /**
     * Simple bridge vertex detection heuristic.
     */
    private boolean isBridgeVertex(Graph<V, DefaultEdge> graph, V vertex, Set<V> neighbors) {
        if (neighbors.size() < 2) {
            return false;
        }

        // Check if removal would significantly disconnect the neighborhood
        long interNeighborConnections = neighbors.parallelStream()
                        .mapToLong(n1 -> neighbors.parallelStream()
                                .filter(n2 -> !n1.equals(n2))
                                .filter(n2 -> graph.containsEdge(n1, n2) || graph.containsEdge(n2, n1))
                                .count())
                        .sum()
                / 2; // Divide by 2 to avoid double counting

        double expectedConnections = neighbors.size() * (neighbors.size() - 1) / 2.0;
        return interNeighborConnections < expectedConnections * 0.3; // Less than 30% connected
    }

    /**
     * Simple articulation point detection heuristic.
     */
    private boolean isLikelyArticulationPoint(Graph<V, DefaultEdge> graph, V vertex, Set<V> neighbors) {
        return neighbors.size() >= 3 && isBridgeVertex(graph, vertex, neighbors);
    }

    /**
     * Computes penalty for removing vertices that connect different components.
     */
    private double computeComponentConnectionPenalty(
            Graph<V, DefaultEdge> graph, V vertex, Set<V> neighbors, int targetTreewidth) {
        if (neighbors.size() < 2) {
            return 0.0;
        }

        // Count high-degree neighbors (potential component representatives)
        long highDegreeNeighbors = neighbors.parallelStream()
                .mapToInt(neighbor -> graph.inDegreeOf(neighbor) + graph.outDegreeOf(neighbor))
                .filter(degree -> degree > targetTreewidth)
                .count();

        if (highDegreeNeighbors >= 2) {
            // Vertex connects multiple high-degree components
            return Math.min(0.3, highDegreeNeighbors * 0.1);
        }

        return 0.0;
    }

    /**
     * Utility method to safely shutdown thread pool.
     */
    private void shutdownThreadPool(ForkJoinPool threadPool) {
        threadPool.shutdown();
        try {
            if (!threadPool.awaitTermination(60, TimeUnit.SECONDS)) {
                threadPool.shutdownNow();
            }
        } catch (InterruptedException e) {
            threadPool.shutdownNow();
            Thread.currentThread().interrupt();
        }
    }

    /**
     * Alternative method for adaptive scoring based on current vs target treewidth.
     * TODO: Revisit?
     */
    public ConcurrentHashMap<V, Double> computeAdaptiveVertexRemovalScore(
            Graph<V, DefaultEdge> graph, int targetTreewidth, int currentTreewidth) {
        ConcurrentHashMap<V, Double> baseScores = computeVertexRemovalScore(graph, targetTreewidth);

        if (currentTreewidth <= targetTreewidth) {
            return baseScores; // Already at or below target
        }

        // Apply adaptive scaling based on the gap between current and target treewidth
        double scalingFactor = (double) (currentTreewidth - targetTreewidth) / Math.max(1, targetTreewidth);

        baseScores.entrySet().parallelStream().forEach(entry -> {
            double adjustedScore = entry.getValue() * (1.0 + scalingFactor);
            entry.setValue(Math.min(10.0, adjustedScore));
        });

        return baseScores;
    }

    /**
     * Computes structural importance of a vertex
     */
    private double computeStructuralImportance(Graph<V, DefaultEdge> graph, V vertex, double centrality) {
        int degree = graph.degreeOf(vertex);
        Set<V> neighbors = Graphs.neighborSetOf(graph, vertex);

        // Count triangles involving this vertex
        long triangles = neighbors.parallelStream()
                        .mapToLong(n1 -> neighbors.stream()
                                .filter(n2 -> !n1.equals(n2) && graph.containsEdge(n1, n2))
                                .count())
                        .sum()
                / 2;

        return degree + centrality * 10 + triangles * 0.5;
    }

    /**
     * Computes betweenness centrality for all vertices
     */
    private Map<V, Double> originalComputeBetweennessCentrality(Graph<V, DefaultEdge> graph) {
        Map<V, Double> centrality = new ConcurrentHashMap<>();
        List<V> vertices = new ArrayList<>(graph.vertexSet());

        // Initialize all centralities to 0
        vertices.parallelStream().forEach(v -> centrality.put(v, 0.0));

        // For efficiency, sample pairs of vertices for large graphs
        // sampleSize and random were not used...
        int sampleSize = Math.min(vertices.size() * (vertices.size() - 1) / 2, 1000);
        Random random = new Random(42); // Fixed seed for reproducibility

        vertices.parallelStream().limit(Math.min(50, vertices.size())).forEach(source -> {
            Map<V, List<V>> predecessors = new HashMap<>();
            Map<V, Integer> distances = new HashMap<>();
            Map<V, Integer> pathCounts = new HashMap<>();
            Stack<V> stack = new Stack<>();

            // BFS from source
            Queue<V> queue = new ArrayDeque<>();
            queue.offer(source);
            distances.put(source, 0);
            pathCounts.put(source, 1);

            while (!queue.isEmpty()) {
                V current = queue.poll();
                stack.push(current);

                for (V neighbor : Graphs.neighborListOf(graph, current)) {
                    if (!distances.containsKey(neighbor)) {
                        distances.put(neighbor, distances.get(current) + 1);
                        pathCounts.put(neighbor, 0);
                        queue.offer(neighbor);
                    }

                    if (distances.get(neighbor) == distances.get(current) + 1) {
                        pathCounts.put(neighbor, pathCounts.get(neighbor) + pathCounts.get(current));
                        predecessors
                                .computeIfAbsent(neighbor, k -> new ArrayList<>())
                                .add(current);
                    }
                }
            }

            // Accumulate centrality values
            Map<V, Double> dependency = new HashMap<>();
            vertices.forEach(v -> dependency.put(v, 0.0));

            while (!stack.isEmpty()) {
                V vertex = stack.pop();
                if (predecessors.containsKey(vertex)) {
                    for (V predecessor : predecessors.get(vertex)) {
                        double contribution = (pathCounts.get(predecessor) / (double) pathCounts.get(vertex))
                                * (1.0 + dependency.get(vertex));
                        dependency.put(predecessor, dependency.get(predecessor) + contribution);
                    }
                }

                if (!vertex.equals(source)) {
                    synchronized (centrality) {
                        centrality.put(vertex, centrality.get(vertex) + dependency.get(vertex));
                    }
                }
            }
        });

        return centrality;
    }

    /**
     * Computes approximated betweenness centrality using random sampling.
     *
     * This implementation is based on Brandes' approximation algorithm that uses
     * random sampling of source vertices to approximate betweenness centrality values.
     * Instead of computing shortest paths from all vertices, we sample only a subset
     * to achieve significant speedup while maintaining reasonable accuracy.
     *
     * @return a map containing approximate betweenness centrality values for each vertex
     */
    public Map<V, Double> computeBetweennessCentrality(Graph<V, DefaultEdge> graph) {
        Set<V> vertices = graph.vertexSet();
        int n = vertices.size();

        if (n <= 2) {
            // For very small graphs, return exact computation
            return computeExactBetweennessCentrality(graph);
        }

        // Calculate sample size based on graph characteristics and desired accuracy
        // Using the formula from Riondato & Kornaropoulos and Brandes & Pich research
        double epsilon = 0.1; // Desired approximation error (can be made configurable)
        double delta = 0.1; // Probability of exceeding error bound (can be made configurable)

        // Compute sample size - various strategies exist in literature:
        // 1. Fixed percentage of nodes (simple but effective)
        // 2. Based on graph diameter and error bounds (more theoretical)
        // 3. Adaptive sampling based on convergence

        int sampleSize = Math.min(n, Math.max(10, (int) Math.ceil(
                Math.log(2.0 / delta) / (2 * epsilon * epsilon) * Math.log(n) // Additional factor based on network size
                )));

        // For very large graphs, cap the sample size to ensure efficiency
        if (n > 10000) {
            sampleSize = Math.min(sampleSize, n / 10); // At most 10% of vertices
        }

        System.out.println("Computing approximated betweenness centrality with " + sampleSize + " samples out of " + n
                + " vertices");

        // Initialize betweenness centrality scores
        Map<V, Double> betweenness = new HashMap<>();
        vertices.forEach(v -> betweenness.put(v, 0.0));

        // Random number generator for sampling
        Random random = ThreadLocalRandom.current();

        // Convert vertices to list for random sampling
        List<V> vertexList = new ArrayList<>(vertices);

        // Sample source vertices and compute contributions
        Set<V> sampledSources = sampleSourceVertices(graph, vertexList, sampleSize, random);

        // Compute betweenness contributions from sampled sources
        for (V source : sampledSources) {
            Map<V, Double> contributions = computeSingleSourceBetweennessContributions(graph, source);

            // Add contributions to total betweenness (scaled by sampling factor)
            double scalingFactor = (double) n / sampleSize;
            for (Map.Entry<V, Double> entry : contributions.entrySet()) {
                V vertex = entry.getKey();
                double contribution = entry.getValue() * scalingFactor;
                betweenness.merge(vertex, contribution, Double::sum);
            }
        }

        return betweenness;
    }

    /**
     * Samples source vertices using different strategies based on graph characteristics.
     *
     * @param vertexList list of all vertices
     * @param sampleSize number of vertices to sample
     * @param random random number generator
     * @return set of sampled source vertices
     */
    private Set<V> sampleSourceVertices(
            Graph<V, DefaultEdge> graph, List<V> vertexList, int sampleSize, Random random) {
        Set<V> sampledSources = new HashSet<>();

        // Strategy 1: Degree-weighted sampling (Brandes & Pich approach)
        // Higher degree vertices are more likely to be selected as they lie on more paths
        if (shouldUseDegreeWeightedSampling(graph)) {
            sampledSources = degreeWeightedSampling(graph, vertexList, sampleSize, random);
        }
        // Strategy 2: Uniform random sampling (simpler, often effective)
        else {
            sampledSources = uniformRandomSampling(vertexList, sampleSize, random);
        }

        return sampledSources;
    }

    /**
     * Determines whether to use degree-weighted sampling based on graph characteristics.
     */
    private boolean shouldUseDegreeWeightedSampling(Graph<V, DefaultEdge> graph) {
        // Use degree-weighted sampling for larger, more complex networks
        return graph.vertexSet().size() > 100;
    }

    /**
     * Performs degree-weighted random sampling of source vertices.
     * Vertices with higher degrees have higher probability of being selected.
     */
    private Set<V> degreeWeightedSampling(
            Graph<V, DefaultEdge> graph, List<V> vertexList, int sampleSize, Random random) {
        Set<V> sampledSources = new HashSet<>();

        // Calculate degree weights
        Map<V, Integer> degrees = new HashMap<>();
        int totalDegree = 0;

        for (V vertex : vertexList) {
            int degree = graph.inDegreeOf(vertex) + graph.outDegreeOf(vertex);
            degrees.put(vertex, degree);
            totalDegree += degree;
        }

        // If all vertices have degree 0, fall back to uniform sampling
        if (totalDegree == 0) {
            return uniformRandomSampling(vertexList, sampleSize, random);
        }

        // Sample vertices with probability proportional to their degree
        while (sampledSources.size() < sampleSize && sampledSources.size() < vertexList.size()) {
            double randomValue = random.nextDouble() * totalDegree;
            double cumulativeWeight = 0;

            for (V vertex : vertexList) {
                if (sampledSources.contains(vertex)) continue;

                cumulativeWeight += degrees.get(vertex);
                if (randomValue <= cumulativeWeight) {
                    sampledSources.add(vertex);
                    break;
                }
            }

            // Prevent infinite loop in edge cases
            if (sampledSources.size() == vertexList.size()) break;
        }

        return sampledSources;
    }

    /**
     * Performs uniform random sampling of source vertices.
     */
    private Set<V> uniformRandomSampling(List<V> vertexList, int sampleSize, Random random) {
        Set<V> sampledSources = new HashSet<>();

        // Use reservoir sampling for efficiency
        for (int i = 0; i < Math.min(sampleSize, vertexList.size()); i++) {
            V vertex;
            do {
                vertex = vertexList.get(random.nextInt(vertexList.size()));
            } while (sampledSources.contains(vertex));

            sampledSources.add(vertex);
        }

        return sampledSources;
    }

    /**
     * Computes betweenness centrality contributions from a single source vertex.
     * This is the core Brandes algorithm for single-source shortest paths.
     *
     * @param graph
     * @param source the source vertex
     * @return map of betweenness contributions for each vertex
     */
    private Map<V, Double> computeSingleSourceBetweennessContributions(Graph<V, DefaultEdge> graph, V source) {
        Map<V, Double> contributions = new HashMap<>();
        Map<V, List<V>> predecessors = new HashMap<>();
        Map<V, Double> sigma = new HashMap<>(); // Number of shortest paths
        Map<V, Integer> distance = new HashMap<>();
        Map<V, Double> delta = new HashMap<>(); // Dependency values

        // Initialize
        graph.vertexSet().forEach(v -> {
            predecessors.put(v, new ArrayList<>());
            sigma.put(v, 0.0);
            distance.put(v, -1);
            delta.put(v, 0.0);
            contributions.put(v, 0.0);
        });

        sigma.put(source, 1.0);
        distance.put(source, 0);

        // BFS to find shortest paths and count them
        Queue<V> queue = new LinkedList<>();
        Stack<V> stack = new Stack<>();
        queue.offer(source);

        while (!queue.isEmpty()) {
            V vertex = queue.poll();
            stack.push(vertex);

            // Examine outgoing edges
            for (DefaultEdge edge : graph.outgoingEdgesOf(vertex)) {
                V neighbor = graph.getEdgeTarget(edge);

                // First time visiting neighbor
                if (distance.get(neighbor) < 0) {
                    queue.offer(neighbor);
                    distance.put(neighbor, distance.get(vertex) + 1);
                }

                // Shortest path to neighbor via vertex
                if (distance.get(neighbor).equals(distance.get(vertex) + 1)) {
                    sigma.put(neighbor, sigma.get(neighbor) + sigma.get(vertex));
                    predecessors.get(neighbor).add(vertex);
                }
            }
        }

        // Accumulation phase - compute dependencies
        while (!stack.isEmpty()) {
            V vertex = stack.pop();

            for (V predecessor : predecessors.get(vertex)) {
                double contribution = (sigma.get(predecessor) / sigma.get(vertex)) * (1 + delta.get(vertex));
                delta.put(predecessor, delta.get(predecessor) + contribution);
            }

            if (!vertex.equals(source)) {
                contributions.put(vertex, delta.get(vertex));
            }
        }

        return contributions;
    }

    /**
     * Computes exact betweenness centrality for small graphs or when high precision is needed.
     *
     * @return map of exact betweenness centrality values
     */
    private Map<V, Double> computeExactBetweennessCentrality(Graph<V, DefaultEdge> graph) {
        Map<V, Double> betweenness = new HashMap<>();
        Set<V> vertices = graph.vertexSet();

        // Initialize all betweenness values to 0
        vertices.forEach(v -> betweenness.put(v, 0.0));

        // Compute contributions from each vertex as source
        for (V source : vertices) {
            Map<V, Double> contributions = computeSingleSourceBetweennessContributions(graph, source);

            for (Map.Entry<V, Double> entry : contributions.entrySet()) {
                V vertex = entry.getKey();
                betweenness.merge(vertex, entry.getValue(), Double::sum);
            }
        }

        return betweenness;
    }

    /**
     * Alternative adaptive sampling approach that adjusts sample size based on convergence.
     * This can provide better accuracy guarantees but is more computationally expensive.
     */
    public Map<V, Double> computeBetweennessCentralityAdaptive(Graph<V, DefaultEdge> graph) {
        Set<V> vertices = graph.vertexSet();
        int n = vertices.size();

        Map<V, Double> betweenness = new HashMap<>();
        vertices.forEach(v -> betweenness.put(v, 0.0));

        List<V> vertexList = new ArrayList<>(vertices);
        Random random = ThreadLocalRandom.current();

        int minSamples = Math.max(10, n / 100);
        int maxSamples = Math.min(n, n / 2);

        Map<V, Double> previousBetweenness = new HashMap<>(betweenness);
        double convergenceThreshold = 0.01; // 1% change threshold

        for (int sampleCount = minSamples; sampleCount <= maxSamples; sampleCount += minSamples) {
            // Sample additional vertices
            Set<V> newSamples = uniformRandomSampling(vertexList, minSamples, random);

            // Compute contributions from new samples
            for (V source : newSamples) {
                Map<V, Double> contributions = computeSingleSourceBetweennessContributions(graph, source);
                double scalingFactor = (double) n / sampleCount;

                for (Map.Entry<V, Double> entry : contributions.entrySet()) {
                    V vertex = entry.getKey();
                    double contribution = entry.getValue() * scalingFactor;
                    betweenness.merge(vertex, contribution, Double::sum);
                }
            }

            // Check for convergence
            if (hasConverged(betweenness, previousBetweenness, convergenceThreshold)) {
                System.out.println("Converged after " + sampleCount + " samples");
                break;
            }

            previousBetweenness = new HashMap<>(betweenness);
        }

        return betweenness;
    }

    /**
     * Checks if betweenness centrality values have converged.
     */
    private boolean hasConverged(Map<V, Double> current, Map<V, Double> previous, double threshold) {
        for (V vertex : current.keySet()) {
            double currentValue = current.get(vertex);
            double previousValue = previous.getOrDefault(vertex, 0.0);

            if (previousValue > 0) {
                double relativeChange = Math.abs(currentValue - previousValue) / previousValue;
                if (relativeChange > threshold) {
                    return false;
                }
            } else if (currentValue > threshold) {
                return false; // Significant change from zero
            }
        }
        return true;
    }

    /**
     * Finds articulation points in the graph
     */
    private Set<V> findArticulationPoints(Graph<V, DefaultEdge> graph) {
        Set<V> articulationPoints = ConcurrentHashMap.newKeySet();

        for (V vertex : graph.vertexSet()) {
            // Check if removing this vertex increases number of connected components
            Graph<V, DefaultEdge> testGraph = new DefaultUndirectedGraph<>(DefaultEdge.class);

            // Copy graph without the test vertex
            graph.vertexSet().stream().filter(v -> !v.equals(vertex)).forEach(testGraph::addVertex);

            graph.edgeSet().forEach(edge -> {
                V source = graph.getEdgeSource(edge);
                V target = graph.getEdgeTarget(edge);
                if (!source.equals(vertex) && !target.equals(vertex)) {
                    testGraph.addEdge(source, target);
                }
            });

            // Count connected components
            ConnectivityInspector<V, DefaultEdge> originalInspector = new ConnectivityInspector<>(graph);
            ConnectivityInspector<V, DefaultEdge> testInspector = new ConnectivityInspector<>(testGraph);

            if (testInspector.connectedSets().size()
                    > originalInspector.connectedSets().size()) {
                articulationPoints.add(vertex);
            }
        }

        return articulationPoints;
    }

    /**
     * Computes approximated betweenness centrality using random sampling.
     *
     * This implementation is based on Brandes' approximation algorithm that uses
     * random sampling of source vertices to approximate betweenness centrality values.
     * Instead of computing shortest paths from all vertices, we sample only a subset
     * to achieve significant speedup while maintaining reasonable accuracy.
     *
     * @return a map containing approximate betweenness centrality values for each vertex
     */
    // TODO: Memoize since this method is called twice
    private Map<V, Double> computeBetweennessCentralityParallel(Graph<V, DefaultEdge> graph) {
        Set<V> vertices = graph.vertexSet();
        int n = vertices.size();

        if (n <= 2) {
            // For very small graphs, return exact computation
            return computeExactBetweennessCentralityParallel(graph);
        }

        // Calculate sample size based on graph characteristics and desired accuracy
        double epsilon = 0.1; // Desired approximation error
        double delta = 0.1; // Probability of exceeding error bound

        int initialSampleSize = Math.min(
                n, Math.max(10, (int) Math.ceil(Math.log(2.0 / delta) / (2 * epsilon * epsilon) * Math.log(n))));

        int sampleSize;
        // For very large graphs, cap the sample size
        if (n > 10000) {
            sampleSize = Math.min(initialSampleSize, n / 10);
        } else {
            sampleSize = initialSampleSize;
        }

        System.out.println("Computing approximated betweenness centrality with " + sampleSize + " samples out of " + n
                + " vertices (parallel)");

        // Initialize concurrent betweenness centrality scores
        ConcurrentHashMap<V, Double> betweenness = new ConcurrentHashMap<>();
        vertices.parallelStream().forEach(v -> betweenness.put(v, 0.0));

        // Thread-safe random number generator
        ThreadLocalRandom random = ThreadLocalRandom.current();

        // Convert vertices to concurrent list for thread-safe access
        List<V> vertexList = new CopyOnWriteArrayList<>(vertices);

        // Custom ForkJoinPool for better control over parallelization
        ForkJoinPool customThreadPool = new ForkJoinPool(
                Math.min(
                        Runtime.getRuntime().availableProcessors(),
                        Math.max(1, sampleSize / 10)) // Scale threads based on sample size
                );

        try {
            CompletableFuture<Void> computation = CompletableFuture.runAsync(
                    () -> {
                        // Sample source vertices in parallel
                        Set<V> sampledSources = sampleSourceVerticesParallel(graph, vertexList, sampleSize, random);

                        // Scaling factor for approximation
                        double scalingFactor = (double) n / sampleSize;

                        // Process sampled sources in parallel and accumulate results
                        sampledSources.parallelStream().forEach(source -> {
                            ConcurrentHashMap<V, Double> contributions =
                                    computeSingleSourceBetweennessContributionsParallel(graph, source);

                            // Atomically update betweenness values with scaling
                            contributions.entrySet().parallelStream().forEach(entry -> {
                                V vertex = entry.getKey();
                                double scaledContribution = entry.getValue() * scalingFactor;
                                betweenness.merge(vertex, scaledContribution, Double::sum);
                            });
                        });
                    },
                    customThreadPool);

            // Wait for completion
            computation.get();

        } catch (InterruptedException | ExecutionException e) {
            Thread.currentThread().interrupt();
            throw new RuntimeException("Parallel betweenness centrality computation failed", e);
        } finally {
            customThreadPool.shutdown();
            try {
                if (!customThreadPool.awaitTermination(60, TimeUnit.SECONDS)) {
                    customThreadPool.shutdownNow();
                }
            } catch (InterruptedException e) {
                customThreadPool.shutdownNow();
                Thread.currentThread().interrupt();
            }
        }

        return betweenness;
    }

    /**
     * Samples source vertices using parallel processing with different sampling strategies.
     */
    private Set<V> sampleSourceVerticesParallel(
            Graph<V, DefaultEdge> graph, List<V> vertexList, int sampleSize, ThreadLocalRandom random) {

        if (shouldUseDegreeWeightedSampling(graph)) {
            return degreeWeightedSamplingParallel(graph, vertexList, sampleSize, random);
        } else {
            return uniformRandomSamplingParallel(vertexList, sampleSize, random);
        }
    }

    /**
     * Performs degree-weighted random sampling using parallel streams.
     */
    private Set<V> degreeWeightedSamplingParallel(
            Graph<V, DefaultEdge> graph, List<V> vertexList, int sampleSize, ThreadLocalRandom random) {

        // Calculate degrees in parallel
        ConcurrentMap<V, Integer> degrees = vertexList.parallelStream()
                .collect(Collectors.toConcurrentMap(
                        vertex -> vertex, vertex -> graph.inDegreeOf(vertex) + graph.outDegreeOf(vertex)));

        // Calculate total degree
        int totalDegree =
                degrees.values().parallelStream().mapToInt(Integer::intValue).sum();

        if (totalDegree == 0) {
            return uniformRandomSamplingParallel(vertexList, sampleSize, random);
        }

        // Use concurrent set for thread-safe sampling
        Set<V> sampledSources = ConcurrentHashMap.newKeySet();
        AtomicInteger samplesNeeded = new AtomicInteger(sampleSize);

        // Parallel sampling with retry mechanism
        vertexList.parallelStream().filter(vertex -> samplesNeeded.get() > 0).forEach(vertex -> {
            if (samplesNeeded.get() <= 0 || sampledSources.contains(vertex)) {
                return;
            }

            // Thread-local random for each thread
            ThreadLocalRandom localRandom = ThreadLocalRandom.current();
            double probability = (double) degrees.get(vertex) / totalDegree;

            // Adaptive probability to ensure we get enough samples
            double adjustedProbability = Math.min(1.0, probability * sampleSize * 2.0 / vertexList.size());

            if (localRandom.nextDouble() < adjustedProbability && sampledSources.size() < sampleSize) {

                sampledSources.add(vertex);
                samplesNeeded.decrementAndGet();
            }
        });

        // Fill remaining slots with uniform sampling if needed
        if (sampledSources.size() < sampleSize) {
            Set<V> additionalSamples = vertexList.parallelStream()
                    .filter(vertex -> !sampledSources.contains(vertex))
                    .limit(sampleSize - sampledSources.size())
                    .collect(Collectors.toSet());
            sampledSources.addAll(additionalSamples);
        }

        return sampledSources;
    }

    /**
     * Performs uniform random sampling using parallel streams.
     */
    private Set<V> uniformRandomSamplingParallel(List<V> vertexList, int sampleSize, ThreadLocalRandom random) {

        // Use parallel stream to shuffle and take first sampleSize elements
        return vertexList.parallelStream()
                .unordered() // Allow parallel processing without ordering constraints
                .distinct() // Ensure uniqueness
                .limit(sampleSize)
                .collect(Collectors.toConcurrentMap(
                        vertex -> vertex, vertex -> ThreadLocalRandom.current().nextDouble()))
                .entrySet()
                .parallelStream()
                .sorted(Map.Entry.comparingByValue()) // Sort by random values
                .limit(sampleSize)
                .map(Map.Entry::getKey)
                .collect(Collectors.toSet());
    }

    /**
     * Computes single-source betweenness contributions using parallel processing.
     */
    private ConcurrentHashMap<V, Double> computeSingleSourceBetweennessContributionsParallel(
            Graph<V, DefaultEdge> graph, V source) {

        Set<V> vertices = graph.vertexSet();
        ConcurrentHashMap<V, Double> contributions = new ConcurrentHashMap<>();
        ConcurrentHashMap<V, List<V>> predecessors = new ConcurrentHashMap<>();
        ConcurrentHashMap<V, AtomicDouble> sigma = new ConcurrentHashMap<>();
        ConcurrentHashMap<V, AtomicInteger> distance = new ConcurrentHashMap<>();
        ConcurrentHashMap<V, AtomicDouble> delta = new ConcurrentHashMap<>();

        // Parallel initialization
        vertices.parallelStream().forEach(v -> {
            predecessors.put(v, new CopyOnWriteArrayList<>());
            sigma.put(v, new AtomicDouble(0.0));
            distance.put(v, new AtomicInteger(-1));
            delta.put(v, new AtomicDouble(0.0));
            contributions.put(v, 0.0);
        });

        sigma.get(source).set(1.0);
        distance.get(source).set(0);

        // BFS with level-wise parallel processing
        ConcurrentLinkedQueue<V> currentLevel = new ConcurrentLinkedQueue<>();
        ConcurrentLinkedQueue<V> nextLevel = new ConcurrentLinkedQueue<>();
        ConcurrentLinkedQueue<V> visitOrder = new ConcurrentLinkedQueue<>();

        currentLevel.offer(source);

        while (!currentLevel.isEmpty()) {
            nextLevel.clear();

            // Process current level
            for (V vertex : currentLevel) {
                visitOrder.offer(vertex);

                // Examine outgoing edges
                for (DefaultEdge edge : graph.outgoingEdgesOf(vertex)) {
                    V neighbor = graph.getEdgeTarget(edge);
                    int currentDist = distance.get(vertex).get();

                    // Atomic check and update for first visit
                    if (distance.get(neighbor).compareAndSet(-1, currentDist + 1)) {
                        nextLevel.offer(neighbor);
                    }

                    // Check if this is a shortest path
                    if (distance.get(neighbor).get() == currentDist + 1) {
                        sigma.get(neighbor).addAndGet(sigma.get(vertex).get());
                        predecessors.get(neighbor).add(vertex);
                    }
                }
            }

            // Swap levels
            ConcurrentLinkedQueue<V> temp = currentLevel;
            currentLevel = nextLevel;
            nextLevel = temp;
        }

        // Accumulation phase - process in reverse order
        List<V> reversedOrder = new ArrayList<>(visitOrder);
        Collections.reverse(reversedOrder);

        // Process accumulation in parallel batches to maintain dependencies
        reversedOrder.parallelStream().forEach(vertex -> {
            if (!vertex.equals(source)) {
                // Process predecessors in parallel
                predecessors.get(vertex).parallelStream().forEach(predecessor -> {
                    double sigmaRatio =
                            sigma.get(predecessor).get() / sigma.get(vertex).get();
                    double contribution = sigmaRatio * (1 + delta.get(vertex).get());
                    delta.get(predecessor).addAndGet(contribution);
                });

                contributions.put(vertex, delta.get(vertex).get());
            }
        });

        return contributions;
    }

    /**
     * Computes exact betweenness centrality for small graphs using parallel processing.
     */
    private ConcurrentHashMap<V, Double> computeExactBetweennessCentralityParallel(Graph<V, DefaultEdge> graph) {
        Set<V> vertices = graph.vertexSet();
        ConcurrentHashMap<V, Double> betweenness = new ConcurrentHashMap<>();

        // Initialize in parallel
        vertices.parallelStream().forEach(v -> betweenness.put(v, 0.0));

        // Compute contributions from each vertex as source in parallel
        vertices.parallelStream().forEach(source -> {
            ConcurrentHashMap<V, Double> contributions =
                    computeSingleSourceBetweennessContributionsParallel(graph, source);

            // Atomically merge contributions
            contributions.entrySet().parallelStream().forEach(entry -> {
                betweenness.merge(entry.getKey(), entry.getValue(), Double::sum);
            });
        });

        return betweenness;
    }

    /**
     * Adaptive parallel sampling with convergence detection.
     */
    public ConcurrentHashMap<V, Double> computeBetweennessCentralityAdaptiveParallel(Graph<V, DefaultEdge> graph) {
        Set<V> vertices = graph.vertexSet();
        int n = vertices.size();

        ConcurrentHashMap<V, Double> betweenness = new ConcurrentHashMap<>();
        vertices.parallelStream().forEach(v -> betweenness.put(v, 0.0));

        List<V> vertexList = new CopyOnWriteArrayList<>(vertices);
        AtomicInteger totalSamples = new AtomicInteger(0);

        int minSamples = Math.max(10, n / 100);
        int maxSamples = Math.min(n, n / 2);
        int batchSize = Math.max(1, minSamples / 4);

        ConcurrentHashMap<V, Double> previousBetweenness = new ConcurrentHashMap<>(betweenness);
        double convergenceThreshold = 0.01;

        // Parallel adaptive sampling with convergence checking
        IntStream.range(0, (maxSamples - minSamples) / batchSize + 1)
                .parallel()
                .takeWhile(batchIndex -> {
                    int currentBatchStart = minSamples + batchIndex * batchSize;
                    int currentBatchSize = Math.min(batchSize, maxSamples - currentBatchStart);

                    if (currentBatchSize <= 0) return false;

                    // Sample new batch in parallel
                    Set<V> newSamples =
                            uniformRandomSamplingParallel(vertexList, currentBatchSize, ThreadLocalRandom.current());

                    // Compute contributions from new samples in parallel
                    AtomicInteger currentTotal = new AtomicInteger(totalSamples.addAndGet(currentBatchSize));

                    newSamples.parallelStream().forEach(source -> {
                        ConcurrentHashMap<V, Double> contributions =
                                computeSingleSourceBetweennessContributionsParallel(graph, source);

                        double scalingFactor = (double) n / currentTotal.get();

                        contributions.entrySet().parallelStream().forEach(entry -> {
                            V vertex = entry.getKey();
                            double contribution = entry.getValue() * scalingFactor;
                            betweenness.merge(vertex, contribution, Double::sum);
                        });
                    });

                    // Check convergence in parallel
                    boolean converged = hasConvergedParallel(betweenness, previousBetweenness, convergenceThreshold);

                    if (converged) {
                        System.out.println("Converged after " + currentTotal.get() + " samples (parallel)");
                        return false; // Stop sampling
                    }

                    // Update previous values for next iteration
                    previousBetweenness.clear();
                    betweenness.entrySet().parallelStream()
                            .forEach(entry -> previousBetweenness.put(entry.getKey(), entry.getValue()));

                    return true; // Continue sampling
                })
                .forEach(batchIndex -> {
                    /* Processing handled in takeWhile */
                });

        return betweenness;
    }

    /**
     * Parallel convergence checking.
     */
    private boolean hasConvergedParallel(
            ConcurrentHashMap<V, Double> current, ConcurrentHashMap<V, Double> previous, double threshold) {

        return current.entrySet().parallelStream().allMatch(entry -> {
            V vertex = entry.getKey();
            double currentValue = entry.getValue();
            double previousValue = previous.getOrDefault(vertex, 0.0);

            if (previousValue > 0) {
                double relativeChange = Math.abs(currentValue - previousValue) / previousValue;
                return relativeChange <= threshold;
            } else {
                return currentValue <= threshold;
            }
        });
    }

    /**
     * Utility method to get thread-safe metrics about the sampling process.
     */
    public ConcurrentHashMap<String, Double> getSamplingMetrics(int sampleSize, int totalVertices) {
        ConcurrentHashMap<String, Double> metrics = new ConcurrentHashMap<>();

        metrics.put("sample_ratio", (double) sampleSize / totalVertices);
        metrics.put("expected_speedup", (double) totalVertices / sampleSize);
        metrics.put(
                "parallel_efficiency",
                (double) Runtime.getRuntime().availableProcessors() / Math.max(1, sampleSize / 10));

        return metrics;
    }

    /**
     * Computes quality score for a modulator
     */
    private double computeModulatorQuality(Graph<V, E> graph, Set<V> modulator, int targetTreewidth) {
        int resultingTreewidth = treewidthComputer.computeEta(graph, modulator);

        if (resultingTreewidth > targetTreewidth) {
            return Double.MAX_VALUE; // Invalid solution
        }

        // Quality = size penalty + treewidth penalty
        return modulator.size() + (resultingTreewidth * 0.1);
    }

    /**
     * Converts directed graph to undirected
     */
    private Graph<V, DefaultEdge> convertToUndirected(Graph<V, E> directed) {
        Graph<V, DefaultEdge> undirected = new DefaultUndirectedGraph<>(DefaultEdge.class);

        directed.vertexSet().forEach(undirected::addVertex);

        directed.edgeSet().forEach(edge -> {
            V source = directed.getEdgeSource(edge);
            V target = directed.getEdgeTarget(edge);
            if (!source.equals(target) && !undirected.containsEdge(source, target)) {
                undirected.addEdge(source, target);
            }
        });

        return undirected;
    }

    /**
     * Fallback modulator computation
     */
    private ModulatorResult<V> computeFallbackModulator(Graph<V, E> graph, int targetTreewidth, int maxSize) {
        Set<V> modulator = graph.vertexSet().stream()
                .sorted((v1, v2) -> Integer.compare(
                        graph.inDegreeOf(v2) + graph.outDegreeOf(v2), graph.inDegreeOf(v1) + graph.outDegreeOf(v1)))
                .limit(maxSize)
                .collect(Collectors.toSet());

        return new ModulatorResult<>(
                modulator,
                treewidthComputer.computeEta(graph, modulator),
                computeModulatorQuality(graph, modulator, targetTreewidth));
    }

    private Set<V> getFutureValue(Future<Set<V>> future) {
        try {
            return future.get();
        } catch (Exception e) {
            return null;
        }
    }

    public void shutdown() {
        treewidthComputer.shutdown();
        fvsComputer.shutdown();
        if (executorService != null && !executorService.isShutdown()) {
            executorService.shutdown();
        }
    }

    /**
     * Result container for modulator computation
     */
    public static class ModulatorResult<V> {
        private final Set<V> modulator;
        private final int resultingTreewidth;
        private final double qualityScore;

        public ModulatorResult(Set<V> modulator, int resultingTreewidth, double qualityScore) {
            this.modulator = new HashSet<>(modulator);
            this.resultingTreewidth = resultingTreewidth;
            this.qualityScore = qualityScore;
        }

        public Set<V> getModulator() {
            return new HashSet<>(modulator);
        }

        public int getResultingTreewidth() {
            return resultingTreewidth;
        }

        public double getQualityScore() {
            return qualityScore;
        }

        public int getSize() {
            return modulator.size();
        }

        @Override
        public String toString() {
            return String.format(
                    "ModulatorResult{size=%d, treewidth=%d, quality=%.2f}",
                    modulator.size(), resultingTreewidth, qualityScore);
        }
    }
}
